{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2. Преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель этого задания -- преобразовать имеющиеся атрибуты пользователей в признаки так, чтобы полученная матрица признаков была пригодна для подачи в алгоритм кластеризации. Этап конструирования признаков -- самый важный и обычно самый долгий. К нему возвращаются много раз на протяжении решения задачи анализа данных.\n",
    "\n",
    "Кроме библиотек, использованных в первом задании, нам понадобятся следующие библиотеки:\n",
    "1. [scikit-learn](http://scikit-learn.org/stable/) -- библиотека, реализующая множество алгоритмов машинного обучения и сопутствующих алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import sklearn.preprocessing as sp\n",
    "import csv\n",
    "import re\n",
    "import dateutil\n",
    "\n",
    "np.set_printoptions(linewidth=150, precision=3, suppress=True)\n",
    "\n",
    "# Plotting config\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_parser = lambda date_str: datetime.datetime.strptime(date_str, \"%Y-%m\") if pd.notnull(date_str) and date_str else None\n",
    "df_users = pd.read_csv(\"hw1_out.csv\", sep=\"\\t\", encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC, converters={\"created_at\": ts_parser})\n",
    "# Remove rows with users not found\n",
    "df_users = df_users[pd.notnull(df_users['name'])]\n",
    "df_users[\"lat\"].fillna(value=0, inplace=True)\n",
    "df_users[\"lon\"].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необходимо ввести новые признаки. Для каждого пользователя предлагается ввести следующие признаки:\n",
    "- name_words - количество слов в имени\n",
    "- screen_name_length - количество символов в псевдониме\n",
    "- description_length - длина описания\n",
    "- created_year - год создания аккаунта\n",
    "- country_code - код страны\n",
    "- verified - предлагается перевести в тип int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_new_features(df_users, features):\n",
    "    # Introduce new features\n",
    "    new_features = [\"name_words\", \"screen_name_length\", \"description_length\", \"created_year\", \"country_code\", \"verified\"]\n",
    "    \n",
    "    # Add new_features to features\n",
    "    # place your code here\n",
    "    \n",
    "    # Calculate new features and place them into data frame\n",
    "    # place tour code here\n",
    "    \n",
    "    return df_users, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [\"lat\", \"lon\", \"followers_count\", \"friends_count\", \"statuses_count\", \"favourites_count\", \"listed_count\"]\n",
    "df_users, features = create_new_features(df_users, features)\n",
    "\n",
    "x = df_users[pd.notnull(df_users.cat)][features].values\n",
    "y = df_users[pd.notnull(df_users.cat)][\"cat\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, являются ли какие-либо из выбранных признаков сильно скоррелированными. Для этого посчитаем матрицу корреляций и выберем те пары признаков, фбсолютное значения коэффициента корреляции между которыми больше 0.2. Необходимо реализовать функцию find_correlated_features, в которой нужно рассчитать коэффициенты корелляции и вывести те, которые больше 0.2. Подсказка: предлагается найти необходимую функцию в библиотеке np и реализовать find_correlated_features с использованием не более 5 строк кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_correlated_features(x, features):\n",
    "    # replace this code to find really correlated features\n",
    "    for i, feature_i in enumerate(features):\n",
    "        for j, feature_j in enumerate(features):\n",
    "            if i < j:\n",
    "                print \"Correlated features: %s + %s -> %.2f\" % (feature_i, feature_j, 0.51)\n",
    "                #найти функцию в библиотеках за 1 строку\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    39.76     -98.5     7605.    ...,  50614.       787.       417.   ]\n",
      " [    15.208    145.753   1919.    ...,   7162.       127.        97.   ]\n",
      " [    28.751    -82.5    24780.    ...,   2290.      7802.       216.   ]\n",
      " ..., \n",
      " [    43.667    -71.5      421.    ...,   7002.       902.         2.   ]\n",
      " [     0.         0.       191.    ...,   4758.       953.         7.   ]\n",
      " [    39.334    -82.045    260.    ...,   1111.      1034.         0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlated features: lat + lon -> 0.51\n",
      "Correlated features: lat + followers_count -> 0.51\n",
      "Correlated features: lat + friends_count -> 0.51\n",
      "Correlated features: lat + statuses_count -> 0.51\n",
      "Correlated features: lat + favourites_count -> 0.51\n",
      "Correlated features: lat + listed_count -> 0.51\n",
      "Correlated features: lon + followers_count -> 0.51\n",
      "Correlated features: lon + friends_count -> 0.51\n",
      "Correlated features: lon + statuses_count -> 0.51\n",
      "Correlated features: lon + favourites_count -> 0.51\n",
      "Correlated features: lon + listed_count -> 0.51\n",
      "Correlated features: followers_count + friends_count -> 0.51\n",
      "Correlated features: followers_count + statuses_count -> 0.51\n",
      "Correlated features: followers_count + favourites_count -> 0.51\n",
      "Correlated features: followers_count + listed_count -> 0.51\n",
      "Correlated features: friends_count + statuses_count -> 0.51\n",
      "Correlated features: friends_count + favourites_count -> 0.51\n",
      "Correlated features: friends_count + listed_count -> 0.51\n",
      "Correlated features: statuses_count + favourites_count -> 0.51\n",
      "Correlated features: statuses_count + listed_count -> 0.51\n",
      "Correlated features: favourites_count + listed_count -> 0.51\n"
     ]
    }
   ],
   "source": [
    "find_correlated_features(x, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделилось 3 группы признаков:\n",
    "1. Основанные на географии:  \"lat\", \"lon\", \"country_code\"\n",
    "2. Основанные на социальной активности:  \"verified\", \"followers_count\", \"friends_count\", \"statuses_count\", \"favourites_count\", \"listed_count\", \"created_year\"\n",
    "3. Остальные:  \"name_words\", \"screen_name_length\", \"description_length\"\n",
    "\n",
    "Построим взаимные распределения пар признаков в каждой из групп, а также гистограмму значений каждого из признаков с учетом целевой переменной."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Необходимо реалищовать функции: plot_two_features_scatter для построения взаимного распределения пары признаков, plot_feature_histogram для построения гистограммы значений, plot_dataset для построения набора графиков по разным парам признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_two_features_scatter(x_i, x_j, y):\n",
    "    \n",
    "    # Set colors and plot scatter\n",
    "    # your code here\n",
    "    \n",
    "    pass    \n",
    "\n",
    "    \n",
    "def plot_feature_histogram(x_i, y):\n",
    "    \n",
    "    # Compute positive and negative histograms\n",
    "    # your code here\n",
    "    \n",
    "    # Plot stacked barplots\n",
    "    # your code here\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_dataset(x, y, features):\n",
    "    # Tune your plot if necessary\n",
    "    # your code here\n",
    "\n",
    "    for i, feature_i in enumerate(features):\n",
    "        for j, feature_j in enumerate(features):\n",
    "            \n",
    "            # Tune your plot if necessary (for example set labels)\n",
    "            # your code here\n",
    "            # Вероятно, не нужно. Для красоты\n",
    "            # Do actual plotting\n",
    "            if i != j:\n",
    "                plot_two_features_scatter(x[:, i], x[:, j], y)            \n",
    "            else:\n",
    "                plot_feature_histogram(x[:, i], y)\n",
    "    \n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим попарные распределения географических признаков ([подсказка](http://anokhin.github.io/img/hw2_geo.png))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_features_new = [\"lat\", \"lon\", \"country_code\"]\n",
    "geo_features = [f for f in geo_features_new if f in features]\n",
    "\n",
    "geo_feature_ind = [i for i, f in enumerate(features) if f in geo_features]\n",
    "plot_dataset(x[:, geo_feature_ind], y, geo_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Четко видны очертания карты и то, что большинство пользователей происходят из небольшого набора стран. Если принять во внимание конечную цель -- кластеризацию пользователей -- логично предположить, что использование географических признаков для описания пользователя может оказаться не очень полезным. Причина в том, что эти признаки четко пространственно разделены (как минимум, океанами и морями). Поэтому мы рискуем вместо \"интересной\" кластеризации получить просто кластеры, которые будут представлять разные страны. В дальнейшем мы исключим географические признаки из рассмотрения при кластеризации пользователей.\n",
    "\n",
    "Далее построим попарные распределения социальных признаков ([подсказка](http://anokhin.github.io/img/hw2_social1.png))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "social_features_new = [\"verified\", \"followers_count\", \"friends_count\", \"statuses_count\", \"favourites_count\", \"listed_count\", \"created_year\"]\n",
    "social_features = [f for f in social_features_new if f in features]\n",
    "social_feature_ind = [i for i, f in enumerate(features) if f in social_features]\n",
    "plot_dataset(x[:, social_feature_ind], y, social_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графиков видно, что признаки \"followers_count\", \"friends_count\", \"statuses_count\", \"favourites_count\", \"listed_count\" сильно смещены в сторону небольших значений. В таком случае удобно сделать логарифмическое преобразрвание этих признаков, то есть применить к их значениям $x_{ij}$ функцию $\\log(1 + x_{ij})$. Сделаем это и построим новые распределения ([подсказка](http://anokhin.github.io/img/hw2_social2.png)). Необходимо реализовать функцию log_transform_features, которая выполняет указанное логарифмическое преобразование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_transform_features(data, features, transformed_features):\n",
    "    # place your code here\n",
    "    # transform selected features with log function\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed_features = [\"followers_count\", \"friends_count\", \"statuses_count\", \"favourites_count\", \"listed_count\"]\n",
    "x = log_transform_features(x, features, transformed_features)\n",
    "\n",
    "# Re-plot features\n",
    "plot_dataset(x[:, social_feature_ind], y, social_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу бросается в глаза, что признак \"verified\" сильно смещен -- верифицированных пользователей очень мало. Более того, все верифицированные пользователи имеют много фолловеров, поэтому часть информации о верификации дублируется в признаке \"followers_count\". По этой причине в дальнейшем не будем рассмтаривать признак \"verified\".\n",
    "\n",
    "После того как мы с помощью логарифмического преобразования избавились от сильной скошенности признаков, можно наблюдать некоторые интересные зависимости. Например, пользователи, имеющие много фолловеров, обязательно имеют много статусов. Следовательно, чтобы стать популярным, обязательно нужно много писать. Анализ других зависимостей остается как упражнение.\n",
    "\n",
    "Наконец построим попарные распределения остальных признаков ([подсказка](http://anokhin.github.io/img/hw2_other.png))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_features_new = [\"name_words\", \"screen_name_length\", \"description_length\"]\n",
    "other_features = [f for f in other_features_new if f in features]\n",
    "other_feature_ind = [i for i, f in enumerate(features) if f in other_features]\n",
    "plot_dataset(x[:, other_feature_ind], y, other_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак после первичной обработки данных мы имеем 9 числовых признаков, каждый из которых распределен в некотором своем интервале. Для того, чтобы ни один признак не получил перевеса при кластеризации, нормализуем данные так, что каждый признак распределен на отрезке $[0, 1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['created_year' 'name_words' 'screen_name_length' 'description_length'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1612ed03ea96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                      \"listed_count\", \"created_year\", \"name_words\", \"screen_name_length\", \"description_length\"]\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1964\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1965\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2005\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['created_year' 'name_words' 'screen_name_length' 'description_length'] not in index\""
     ]
    }
   ],
   "source": [
    "selected_features = [\"followers_count\", \"friends_count\", \"statuses_count\", \"favourites_count\", \n",
    "                     \"listed_count\", \"created_year\", \"name_words\", \"screen_name_length\", \"description_length\"]\n",
    "\n",
    "x_1 = df_users[selected_features].values\n",
    "y = df_users[\"cat\"].values\n",
    "\n",
    "# x_1 = x[:, selected_features_ind]\n",
    "# Replace nan with 0-s\n",
    "# Is there a smarter way?\n",
    "x_1[np.isnan(x_1)] = 0\n",
    "x_min = x_1.min(axis=0)\n",
    "x_max = x_1.max(axis=0)\n",
    "x_new = (x_1 - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упакуем полученную матрицу в pandas DataFrame и сохраним в файл \"hw2_out.csv\". В следующем задании мы будем кластеризовать пользователей на оновании этих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame(data=x_new, index=df_users[\"uid\"], columns=[f for f in selected_features])\n",
    "df_out.to_csv(\"hw2_out.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
